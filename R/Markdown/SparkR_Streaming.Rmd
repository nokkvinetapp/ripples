---
title: "Spark and R for Streaming - workflow"
author: "Nokkvi Dan - NetApp"
date: "5/17/2021"
output: html_document
---

```{r packages, eval = FALSE}
#Packages needed and warnings turned off
library(future, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(sparklyr, warn.conflicts = FALSE)
library(shiny, warn.conflicts = FALSE)
```

This is a markdown which is intended to help implementing R native data scientists get started with streaming in Apache Spark. This document outlines the workflow from start to end with a couple of alternative methods.

If something is unclear it is good to look at the examples in the other markdowns by Nokkvi Dan.

# The workflow

## 1) Open spark connection

```{r connection, eval = FALSE}
# Creates a sc value in environment that is connected to Spark Server
sc <- spark_connect(master = "local", spark_version = "2.4.3") 

#Reset folders
if(file.exists("source")) unlink("source", TRUE) #Reset input folder
if(file.exists("sourceout")) unlink("sourceout", TRUE) #Reset output folder
```

### Create Stream

```{r create dummy, eval = FALSE}
# Generate Source Folder called "source" and inside it is a "stream_1.csv" file
# stream_generate_test(iterations = 1) 
```


## 2) Navigate stream reader to streaming files

```{r navigation, eval = FALSE}
# Read CSV files in a folder called "source"
read_folder <- stream_read_csv(sc, "source")

# We can add "Time stamps":
# read_folder %>% stream_watermark()
```


## 3) Processing data

* Any ML algorithms or data manipulations.
* The transformed data is usually called **process_stream**.

### Some examples of sparklyr for ML algorithms in Apache Spark

* Random forest - `ml_random_forest`
* Decision tree - `ml_decision_tree`
* Gradient boosted trees - `ml_gradient_boosted_trees`
* Logistic regression - `ml_logistic_regression`
* Multilayer perceptrons (neural net) - `ml_multilayer_perceptron`
* Naive Bayes - `ml_naive_bayes`

### Some examples of data manipulations in Apache Spark

* `select` ~ `SELECT`
* `filter` ~ `WHERE`
* `arrange` ~ `ORDER`
* `summarise` ~ `aggregators: sum, min, sd, etc.`
* `mutate` ~ `operators: +, *, log, etc.`
* `group_by` ~ `GROUP BY`
* `sdf_sample` ~ random sample of rows.
* `show_query(process_stream)` ~ Shows queries
* `collect` ~ returns results to R

## 4) Output, Spark Memory or Not

### Spark Memory:

```{r spark memory output, eval = FALSE}
write_output <- stream_write_memory(process_stream, name = "sourceout")
```

### Not Spark Memory:

```{r not spark memory output, eval = FALSE}
write_output <- stream_write_csv(process_stream, name = "sourceout")
```

Other examples:

* `spark_write_parquet` Writes a Parquet file (Usually the best option)
* `spark_write_csv` Writes a CSV file
* `spark_write_json` Writes a JSON file

## 5) Run stream

```{r execution, eval = FALSE}
invisible(future(stream_generate_test(
  #interval = 0.2, iterations = 100
)))
```

## 6) Source Out folder

### Spark Memory:

```{r source out SM, eval = FALSE}
tbl(sc, "sourceout")
```

### Not Spark Memory:

```{r source out not SM, eval = FALSE}
spark_read_csv(sc, "sourceout", memory=FALSE) %>% 
  # group_by(...) %>% 
  tally()
```

* `spark_read_csv` Reads a CSV file 
* `spark_read_json` Reads a JSON file
* `spark_read_parquet` Reads a parquet file [`spark_read_parquet(sc, "data", "path")`]

## 7) Stop and Close

```{r stop and close, eval = FALSE}
stream_stop(write_output)
spark_disconnect(sc)
```

